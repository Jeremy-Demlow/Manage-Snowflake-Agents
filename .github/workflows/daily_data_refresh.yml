# Daily Data Refresh Pipeline
# ============================
# Runs at 5am PST (1pm UTC) daily to:
# 1. Check data integrity and coverage
# 2. Generate incremental ski resort data (with smart backfill)
# 3. Run DBT fact tables
# 4. Refresh semantic views
#
# Recovery options for data corruption:
# - rebuild_from_date: Regenerate data from a specific date
# - full_refresh: Rebuild all DBT models from scratch
# - clear_raw_data: Delete raw data before regenerating (nuclear option)
#
# Uses official Snowflake CLI GitHub Action:
# https://github.com/snowflakedb/snowflake-cli-action

name: Daily Data Refresh

on:
  schedule:
    # 5am PST = 1pm UTC (13:00)
    - cron: '0 13 * * *'

  # Allow manual trigger with recovery options
  workflow_dispatch:
    inputs:
      rebuild_from_date:
        description: 'Rebuild from date (YYYY-MM-DD) - leave empty for auto-detect'
        required: false
        default: ''
      days:
        description: 'Number of days to generate (overrides auto-detect)'
        required: false
        default: ''
      full_refresh:
        description: 'Force full DBT refresh (rebuilds all fact tables)'
        required: false
        default: 'false'
        type: boolean
      clear_raw_data:
        description: 'âš ï¸ DANGER: Clear raw data before rebuild (use with rebuild_from_date)'
        required: false
        default: 'false'
        type: boolean

jobs:
  data-refresh:
    name: Generate Data & Run DBT
    runs-on: ubuntu-latest

    # Snowflake credentials
    env:
      SNOWFLAKE_CONNECTIONS_DEFAULT_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
      SNOWFLAKE_CONNECTIONS_DEFAULT_USER: ${{ secrets.SNOWFLAKE_USER }}
      SNOWFLAKE_CONNECTIONS_DEFAULT_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}
      SNOWFLAKE_CONNECTIONS_DEFAULT_WAREHOUSE: ${{ secrets.SNOWFLAKE_WAREHOUSE }}
      SNOWFLAKE_CONNECTIONS_DEFAULT_DATABASE: SKI_RESORT_DB
      SNOWFLAKE_CONNECTIONS_DEFAULT_SCHEMA: RAW
      SNOWFLAKE_CONNECTIONS_DEFAULT_ROLE: ${{ secrets.SNOWFLAKE_ROLE }}

    steps:
      - name: ðŸ“¥ Checkout Repository
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      # â”€â”€â”€ Snowflake CLI Setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: ðŸ”§ Setup Snowflake CLI
        uses: snowflakedb/snowflake-cli-action@v1.5
        with:
          cli-version: "latest"
          default-config-file-path: "config.toml"

      - name: âœ… Test Snowflake Connection
        run: |
          echo "Testing Snowflake connection..."
          snow --version
          snow connection test
          echo "âœ… Connection successful!"

      # â”€â”€â”€ Python Setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: ðŸ Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: ðŸ“¦ Install Python Dependencies
        run: |
          pip install --upgrade pip
          pip install "snowflake-snowpark-python[pandas]" pandas pyarrow numpy pyyaml toml pydantic cryptography

      # â”€â”€â”€ Data Integrity Check â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: ðŸ” Check Data Integrity & Coverage
        id: check_data
        run: |
          echo "ðŸ“Š Checking data integrity and coverage..."

          # Get data stats
          snow sql -q "
            SELECT
              MIN(VISIT_DATE) as first_date,
              MAX(VISIT_DATE) as last_date,
              COUNT(DISTINCT VISIT_DATE) as total_days,
              COUNT(*) as total_records
            FROM SKI_RESORT_DB.RAW.PASS_USAGE
          " --format json > /tmp/data_stats.json

          # Parse results
          FIRST_DATE=$(python3 -c "import json; data=json.load(open('/tmp/data_stats.json')); print(data[0]['FIRST_DATE'] if data and data[0]['FIRST_DATE'] else '2025-12-01')")
          LAST_DATE=$(python3 -c "import json; data=json.load(open('/tmp/data_stats.json')); print(data[0]['LAST_DATE'] if data and data[0]['LAST_DATE'] else '2025-12-01')")
          TOTAL_DAYS=$(python3 -c "import json; data=json.load(open('/tmp/data_stats.json')); print(data[0]['TOTAL_DAYS'] if data and data[0]['TOTAL_DAYS'] else 0)")
          TOTAL_RECORDS=$(python3 -c "import json; data=json.load(open('/tmp/data_stats.json')); print(data[0]['TOTAL_RECORDS'] if data and data[0]['TOTAL_RECORDS'] else 0)")

          echo "ðŸ“ˆ Current Data Status:"
          echo "   First date: $FIRST_DATE"
          echo "   Last date:  $LAST_DATE"
          echo "   Total days: $TOTAL_DAYS"
          echo "   Total records: $TOTAL_RECORDS"

          # Check for gaps in data
          echo ""
          echo "ðŸ” Checking for data gaps..."
          snow sql -q "
            WITH date_range AS (
              SELECT DATEADD(day, SEQ4(), '$FIRST_DATE'::DATE) as expected_date
              FROM TABLE(GENERATOR(ROWCOUNT => 365))
              WHERE expected_date <= '$LAST_DATE'::DATE
            ),
            actual_dates AS (
              SELECT DISTINCT VISIT_DATE FROM SKI_RESORT_DB.RAW.PASS_USAGE
            )
            SELECT expected_date as missing_date
            FROM date_range
            LEFT JOIN actual_dates ON date_range.expected_date = actual_dates.VISIT_DATE
            WHERE actual_dates.VISIT_DATE IS NULL
            ORDER BY expected_date
            LIMIT 10
          " --format json > /tmp/gaps.json

          GAPS=$(python3 -c "import json; data=json.load(open('/tmp/gaps.json')); print(','.join([d['MISSING_DATE'] for d in data]) if data else 'none')")

          if [ "$GAPS" != "none" ] && [ -n "$GAPS" ]; then
            echo "âš ï¸  Data gaps detected: $GAPS"
            echo "has_gaps=true" >> $GITHUB_OUTPUT
            echo "gap_dates=$GAPS" >> $GITHUB_OUTPUT
          else
            echo "âœ… No data gaps detected"
            echo "has_gaps=false" >> $GITHUB_OUTPUT
          fi

          # Set outputs
          echo "first_date=$FIRST_DATE" >> $GITHUB_OUTPUT
          echo "last_date=$LAST_DATE" >> $GITHUB_OUTPUT
          echo "total_days=$TOTAL_DAYS" >> $GITHUB_OUTPUT
          echo "total_records=$TOTAL_RECORDS" >> $GITHUB_OUTPUT

      # â”€â”€â”€ Calculate What Needs Generating â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: ðŸ“… Calculate Data Generation Plan
        id: plan
        run: |
          TODAY=$(date +%Y-%m-%d)
          LAST_DATE="${{ steps.check_data.outputs.last_date }}"

          # Check if manual rebuild requested
          REBUILD_FROM="${{ github.event.inputs.rebuild_from_date }}"
          MANUAL_DAYS="${{ github.event.inputs.days }}"

          if [ -n "$REBUILD_FROM" ]; then
            # Manual rebuild from specific date
            START_DATE="$REBUILD_FROM"
            DAYS=$(python3 -c "from datetime import datetime; start = datetime.strptime('$START_DATE', '%Y-%m-%d'); today = datetime.strptime('$TODAY', '%Y-%m-%d'); print(max((today - start).days + 1, 0))")
            echo "ðŸ”§ Manual rebuild from $START_DATE ($DAYS days)"
          elif [ -n "$MANUAL_DAYS" ]; then
            # Manual days specified
            DAYS="$MANUAL_DAYS"
            START_DATE=$(python3 -c "from datetime import datetime, timedelta; today = datetime.strptime('$TODAY', '%Y-%m-%d'); start = today - timedelta(days=$DAYS - 1); print(start.strftime('%Y-%m-%d'))")
            echo "ðŸ”§ Manual: $DAYS days from $START_DATE"
          else
            # Auto-detect: generate from last_date+1 to today
            # If last_date >= today, no generation needed (days will be 0 or negative)
            START_DATE=$(python3 -c "from datetime import datetime, timedelta; last = datetime.strptime('$LAST_DATE', '%Y-%m-%d'); start = last + timedelta(days=1); print(start.strftime('%Y-%m-%d'))")
            DAYS=$(python3 -c "from datetime import datetime; start = datetime.strptime('$START_DATE', '%Y-%m-%d'); today = datetime.strptime('$TODAY', '%Y-%m-%d'); days = (today - start).days + 1; print(max(days, 0))")

            if [ "$DAYS" -le 0 ]; then
              echo "âœ… Data is already up to date (last: $LAST_DATE, today: $TODAY)"
              DAYS=0
            else
              echo "ðŸ¤– Auto-detected: $DAYS days from $START_DATE"
            fi
          fi

          echo "start_date=$START_DATE" >> $GITHUB_OUTPUT
          echo "days=$DAYS" >> $GITHUB_OUTPUT
          echo "today=$TODAY" >> $GITHUB_OUTPUT
          echo "skip_generation=$( [ "$DAYS" -le 0 ] && echo 'true' || echo 'false' )" >> $GITHUB_OUTPUT

          echo ""
          echo "ðŸ“‹ Generation Plan:"
          echo "   Start date: $START_DATE"
          echo "   Days to generate: $DAYS"
          echo "   End date: $TODAY"
          if [ "$DAYS" -le 0 ]; then
            echo "   Status: âœ… Data already current, skipping generation"
          fi

      # â”€â”€â”€ Clear Raw Data (Optional - for corruption recovery) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: ðŸ—‘ï¸ Clear Raw Data (if requested)
        if: github.event.inputs.clear_raw_data == 'true'
        run: |
          echo "âš ï¸  CLEARING RAW DATA from ${{ steps.plan.outputs.start_date }} onwards..."

          snow sql -q "
            DELETE FROM SKI_RESORT_DB.RAW.PASS_USAGE WHERE VISIT_DATE >= '${{ steps.plan.outputs.start_date }}'::DATE;
            DELETE FROM SKI_RESORT_DB.RAW.LIFT_SCANS WHERE SCAN_DATE >= '${{ steps.plan.outputs.start_date }}'::DATE;
            DELETE FROM SKI_RESORT_DB.RAW.TICKET_SALES WHERE PURCHASE_DATE >= '${{ steps.plan.outputs.start_date }}'::DATE;
            DELETE FROM SKI_RESORT_DB.RAW.FOOD_BEVERAGE WHERE TRANSACTION_DATE >= '${{ steps.plan.outputs.start_date }}'::DATE;
            DELETE FROM SKI_RESORT_DB.RAW.RENTALS WHERE RENTAL_DATE >= '${{ steps.plan.outputs.start_date }}'::DATE;
            DELETE FROM SKI_RESORT_DB.RAW.WEATHER_CONDITIONS WHERE WEATHER_DATE >= '${{ steps.plan.outputs.start_date }}'::DATE;
            DELETE FROM SKI_RESORT_DB.RAW.STAFFING_SCHEDULE WHERE SCHEDULE_DATE >= '${{ steps.plan.outputs.start_date }}'::DATE;
            DELETE FROM SKI_RESORT_DB.RAW.SKI_LESSONS WHERE LESSON_DATE >= '${{ steps.plan.outputs.start_date }}'::DATE;
            DELETE FROM SKI_RESORT_DB.RAW.INCIDENTS WHERE INCIDENT_DATE >= '${{ steps.plan.outputs.start_date }}'::DATE;
            DELETE FROM SKI_RESORT_DB.RAW.CUSTOMER_FEEDBACK WHERE FEEDBACK_DATE >= '${{ steps.plan.outputs.start_date }}'::DATE;
            DELETE FROM SKI_RESORT_DB.RAW.PARKING_OCCUPANCY WHERE RECORDED_DATE >= '${{ steps.plan.outputs.start_date }}'::DATE;
            DELETE FROM SKI_RESORT_DB.RAW.GROOMING_LOGS WHERE GROOMING_DATE >= '${{ steps.plan.outputs.start_date }}'::DATE;
          "

          echo "âœ… Raw data cleared"

      # â”€â”€â”€ Generate Data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: ðŸ“Š Generate Incremental Data
        if: steps.plan.outputs.skip_generation != 'true'
        working-directory: data_generation
        env:
          SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
          SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}
          SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}
          SNOWFLAKE_WAREHOUSE: ${{ secrets.SNOWFLAKE_WAREHOUSE }}
          SNOWFLAKE_ROLE: ${{ secrets.SNOWFLAKE_ROLE }}
          SNOWFLAKE_DATABASE: SKI_RESORT_DB
          SNOWFLAKE_SCHEMA: RAW
        run: |
          DAYS="${{ steps.plan.outputs.days }}"
          START_DATE="${{ steps.plan.outputs.start_date }}"

          echo "ðŸŽ¿ Generating $DAYS day(s) of ski resort data starting from $START_DATE..."
          python generate_daily_increment.py \
            --date "$START_DATE" \
            --days "$DAYS" \
            --connection default

      - name: â­ï¸ Skip Data Generation
        if: steps.plan.outputs.skip_generation == 'true'
        run: |
          echo "âœ… Data is already current (last date: ${{ steps.check_data.outputs.last_date }}), skipping generation"

      # â”€â”€â”€ DBT Setup & Run â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: ðŸ“¦ Install DBT
        run: pip install dbt-snowflake

      - name: ðŸ”§ Configure DBT Profile
        run: echo "Using project profiles.yml with env vars"

      - name: ðŸ“¦ Install DBT Dependencies
        working-directory: dbt_ski_resort
        env:
          SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}
        run: |
          echo "ðŸ“¦ Installing DBT dependencies..."
          dbt deps

      - name: ðŸ—ï¸ Run DBT Fact Tables
        working-directory: dbt_ski_resort
        env:
          SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}
        run: |
          echo "ðŸ“ˆ Running DBT fact models..."
          if [ "${{ github.event.inputs.full_refresh }}" = "true" ]; then
            echo "âš ï¸  Full refresh requested - rebuilding all fact tables..."
            dbt run --select "marts.facts" --full-refresh
          else
            dbt run --select "marts.facts"
          fi

      - name: ðŸŽ¯ Run DBT Semantic Views
        working-directory: dbt_ski_resort
        env:
          SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}
        run: |
          echo "ðŸ”„ Refreshing semantic views..."
          dbt run --select "marts.semantic"

      # â”€â”€â”€ Verification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: âœ… Verify Data Load
        id: verify
        run: |
          echo "ðŸ“‹ Verifying data refresh..."

          snow sql -q "
            SELECT VISIT_DATE, COUNT(*) as visitors
            FROM SKI_RESORT_DB.RAW.PASS_USAGE
            WHERE VISIT_DATE >= DATEADD(day, -7, CURRENT_DATE())
            GROUP BY VISIT_DATE
            ORDER BY VISIT_DATE DESC
            LIMIT 7
          "

          # Get final stats
          snow sql -q "
            SELECT
              MIN(VISIT_DATE) as first_date,
              MAX(VISIT_DATE) as last_date,
              COUNT(DISTINCT VISIT_DATE) as total_days,
              COUNT(*) as total_records
            FROM SKI_RESORT_DB.RAW.PASS_USAGE
          " --format json > /tmp/final_stats.json

          FINAL_LAST=$(python3 -c "import json; data=json.load(open('/tmp/final_stats.json')); print(data[0]['LAST_DATE'])")
          FINAL_DAYS=$(python3 -c "import json; data=json.load(open('/tmp/final_stats.json')); print(data[0]['TOTAL_DAYS'])")
          FINAL_RECORDS=$(python3 -c "import json; data=json.load(open('/tmp/final_stats.json')); print(data[0]['TOTAL_RECORDS'])")

          echo "final_last_date=$FINAL_LAST" >> $GITHUB_OUTPUT
          echo "final_total_days=$FINAL_DAYS" >> $GITHUB_OUTPUT
          echo "final_total_records=$FINAL_RECORDS" >> $GITHUB_OUTPUT

      # â”€â”€â”€ Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: ðŸ“ Job Summary
        run: |
          echo "## ðŸŽ¿ Daily Data Refresh Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Run Date:** $(date '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "### ðŸ“Š Data Status Before:" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| First Date | ${{ steps.check_data.outputs.first_date }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Last Date | ${{ steps.check_data.outputs.last_date }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Total Days | ${{ steps.check_data.outputs.total_days }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Total Records | ${{ steps.check_data.outputs.total_records }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Gaps Detected | ${{ steps.check_data.outputs.has_gaps }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "### ðŸ“… Generation Plan:" >> $GITHUB_STEP_SUMMARY
          echo "- **Start Date:** ${{ steps.plan.outputs.start_date }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Days Generated:** ${{ steps.plan.outputs.days }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Full Refresh:** ${{ github.event.inputs.full_refresh || 'false' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Clear Raw Data:** ${{ github.event.inputs.clear_raw_data || 'false' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "### âœ… Data Status After:" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Last Date | ${{ steps.verify.outputs.final_last_date }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Total Days | ${{ steps.verify.outputs.final_total_days }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Total Records | ${{ steps.verify.outputs.final_total_records }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "### ðŸ”§ Recovery Options:" >> $GITHUB_STEP_SUMMARY
          echo "If you need to fix data issues, trigger manually with:" >> $GITHUB_STEP_SUMMARY
          echo "- \`rebuild_from_date\`: Regenerate from a specific date (e.g., 2025-12-01)" >> $GITHUB_STEP_SUMMARY
          echo "- \`full_refresh\`: Rebuild all DBT fact tables" >> $GITHUB_STEP_SUMMARY
          echo "- \`clear_raw_data\`: âš ï¸ Delete and regenerate raw data" >> $GITHUB_STEP_SUMMARY
